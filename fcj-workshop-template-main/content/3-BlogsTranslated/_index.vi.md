---
title: "Các Blog Đã Dịch"
date: 2025-09-16
weight: 300
chapter: false
pre: " <b> 3. </b> "
---

Phần này sẽ liệt kê và giới thiệu các blog bạn đã dịch. Ví dụ:

###  [Blog 1 - Cách Smartsheet Giảm Độ Trễ và Tối Ưu Chi Phí trong Kiến Trúc Serverless](3.1-Blog1)
Bài viết này trình bày một case study thực tế của Smartsheet - nền tảng quản lý công việc doanh nghiệp hàng đầu - về cách họ tối ưu hóa kiến trúc serverless để đạt được **giảm 83% độ trễ P95** và tối ưu hóa chi phí. Thông qua việc triển khai provisioned concurrency với auto scaling và di chuyển sang AWS Graviton, Smartsheet đã giải quyết thành công vấn đề cold start trong môi trường xử lý sự kiện thời gian thực xử lý hàng chục nghìn sự kiện mỗi giây. Bài viết cung cấp thông tin chi tiết về kiến trúc hướng sự kiện, thách thức về độ trễ trong nền tảng cộng tác, và một bản thiết kế tối ưu hóa có thể được áp dụng bởi các tổ chức khác triển khai kiến trúc serverless quy mô doanh nghiệp.

###  [Blog 2 - Di Chuyển và Hiện Đại Hóa VMware Workloads với AWS Transform for VMware](3.2-Blog2/)
Bài viết này khám phá AWS Transform for VMware, một dịch vụ cách mạng được công bố vào ngày 15 tháng 5 năm 2025, giúp đơn giản hóa và tăng tốc việc di chuyển VMware workloads lên AWS Cloud. Blog đề cập đến các thách thức của việc di chuyển đám mây doanh nghiệp, bao gồm các phụ thuộc phức tạp, tài liệu nghèo nàn và tính liên tục vận hành. Nó cung cấp thông tin chi tiết về kiến trúc giải pháp, bao gồm khám phá và đánh giá hợp lý bằng RVTools và AWS Application Discovery Service, chuyển đổi mạng thông minh với các template CloudFormation tự động, lập kế hoạch wave bằng graph neural networks, bảo mật nâng cao với AWS KMS và CloudTrail, và thực hiện di chuyển được điều phối với AWS Application Migration Service sang các EC2 instances.

###  [Blog 3 - Triển Khai LLMs trên Amazon EKS sử dụng vLLM Deep Learning Containers](3.3-Blog3/)
Blog này giới thiệu cách triển khai các mô hình ngôn ngữ lớn (LLMs) trên Amazon EKS bằng cách sử dụng vLLM Deep Learning Containers. Bạn sẽ học được tại sao việc triển khai LLMs gặp nhiều thách thức liên quan đến tối ưu hóa GPU, quản lý mạng và truy cập model weights, cách vLLM AWS DLCs giúp đơn giản hóa việc triển khai với môi trường được tối ưu hóa trước (bao gồm drivers, libraries, hỗ trợ EFA cho multi-node inference). Bài viết cũng hướng dẫn bạn qua các bước triển khai mô hình DeepSeek-R1-Distill-Qwen-32B trên kiến trúc kết hợp Amazon EKS, GPU instances (P4d.24xlarge), Elastic Fabric Adapter, FSx for Lustre storage, và LeaderWorkerSet pattern, tạo ra một hệ thống inference với độ trễ thấp, throughput cao và chi phí vận hành tối thiểu.

###  [Blog 4 - Đơn Giản Hóa AI Operations với Kiến Trúc Tham Khảo Multi-Provider Generative AI Gateway](3.4-Blog4/)
Blog này giới thiệu kiến trúc tham khảo Multi-Provider Generative AI Gateway để đơn giản hóa việc quản lý AI operations. Bạn sẽ học được tại sao các tổ chức gặp khó khăn khi quản lý nhiều AI providers (Amazon Bedrock, SageMaker, OpenAI, Anthropic...) với các thách thức bao gồm phân mảnh providers, quản trị phi tập trung, độ phức tạp vận hành, quản lý chi phí và bảo mật. Giải pháp sử dụng LiteLLM (open source) kết hợp các dịch vụ AWS để tạo một gateway tập trung. Bài viết hướng dẫn bạn qua các tùy chọn triển khai (ECS, EKS), kiến trúc mạng (public global, regional, private VPC), và các tính năng quản trị toàn diện bao gồm: quản lý user/team, API keys, budget control, load balancing, failover, rate limiting, model access control, và tích hợp CloudWatch để monitoring. Gateway cũng hỗ trợ Amazon SageMaker để mở rộng khả năng truy cập custom models, cung cấp một giải pháp quản trị AI thống nhất, an toàn và có khả năng mở rộng cao.

###  [Blog 5 - Cách Smartsheet Nâng Cao Năng Suất Lập Trình Viên với Amazon Bedrock và Roo Code](3.5-Blog5/)
Blog này giới thiệu cách Smartsheet nâng cao năng suất lập trình viên bằng Amazon Bedrock và Roo Code. Bạn sẽ học được về Roo Code - một AI coding assistant tự động tích hợp trực tiếp vào editor, và Amazon Bedrock prompt caching - công nghệ cache các prompts thường dùng giúp giảm chi phí lên đến 90% và giảm độ trễ 85%. Smartsheet đạt được 60% giảm chi phí vận hành và 20% cải thiện latency. Bài viết trình bày các case studies thực tế: tạo tài liệu code trong 4 giờ thay vì 2 tuần, xây dựng công cụ phân tích AWS cost trong 30 phút giúp tiết kiệm $450K/năm. Giải pháp hoạt động bằng cách tách nội dung tĩnh (system prompts, code context) và động (user queries), cache nội dung tĩnh trong 5 phút và tái sử dụng cho các queries tiếp theo. Kết quả cho thấy 70% tổng input được phục vụ từ cache, với 83% giảm chi phí cho follow-up queries. Bài viết cũng chia sẻ best practices bao gồm: triển khai caching sớm, tối ưu developer experience, và monitoring liên tục.

###  [Blog 6 - Sử Dụng Generative AI trên AWS để Phân Tích Hiệu Quả Tài Liệu Lâm Sàng](3.6-Blog6/)
Blog này giới thiệu cách sử dụng generative AI trên AWS để phân tích hiệu quả các tài liệu lâm sàng. Bạn sẽ học được về thách thức trong ngành dược phẩm: quy trình phê duyệt thuốc mất 10-12 năm với 1 năm dành cho study startup, việc xem xét các protocol documents phức tạp thường mất nhiều tuần hoặc tháng. Clario - nhà cung cấp giải pháp dữ liệu cho thử nghiệm lâm sàng - đã xây dựng nền tảng AI trên AWS dựa trên bốn trụ cột: Parsing (dùng Amazon Textract, Comprehend để trích xuất text/images/tables), Retrieval (dùng embedding mo
---
title: "Translated Blogs"
date: 2025-09-16
weight: 300
chapter: false
pre: " <b> 3. </b> "
---

This section will list and introduce the blogs you have translated. For example:

###  [Blog 1 - How Smartsheet Reduced Latency and Optimized Costs in Their Serverless Architecture](3.1-Blog1)
This article presents a real-world case study of Smartsheet - a leading enterprise work management platform - on how they optimized their serverless architecture to achieve an **83% reduction in P95 latency** and cost optimization. Through implementing provisioned concurrency with auto scaling and migrating to AWS Graviton, Smartsheet successfully addressed cold start issues in a real-time event processing environment handling tens of thousands of events per second. The article provides detailed insights into event-driven architecture, latency challenges in collaborative platforms, and an optimization blueprint that can be applied by other organizations deploying enterprise-scale serverless architectures.

###  [Blog 2 - Migrate and Modernize VMware Workloads with AWS Transform for VMware](3.2-Blog2/)
This article explores AWS Transform for VMware, a revolutionary service announced on May 15, 2025, that simplifies and accelerates the migration of VMware workloads to AWS Cloud. The blog addresses the challenges of enterprise cloud migration, including complex dependencies, poor documentation, and operational continuity. It provides detailed insights into the solution architecture, covering streamlined discovery and assessment using RVTools and AWS Application Discovery Service, intelligent network conversion with automated CloudFormation templates, wave planning using graph neural networks, enhanced security with AWS KMS and CloudTrail, and orchestrated migration execution with AWS Application Migration Service to EC2 instances.

###  [Blog 3 - Deploy LLMs on Amazon EKS using vLLM Deep Learning Containers](3.3-Blog3/)
This blog introduces how to deploy large language models (LLMs) on Amazon EKS using vLLM Deep Learning Containers. You will learn why deploying LLMs faces many challenges related to GPU optimization, network management, and model weights access, how vLLM AWS DLCs help simplify deployment with a pre-optimized environment (including drivers, libraries, EFA support for multi-node inference). The article also guides you through the steps to deploy the DeepSeek-R1-Distill-Qwen-32B model on an architecture combining Amazon EKS, GPU instances (P4d.24xlarge), Elastic Fabric Adapter, FSx for Lustre storage, and LeaderWorkerSet pattern, creating an inference system with low latency, high throughput, and minimal operational costs.

#  [Blog 4 - Simplify AI operations with the Multi-Provider Generative AI Gateway reference architecture](3.4-Blog4/)
TThis blog introduces the Multi-Provider Generative AI Gateway reference architecture to simplify AI operations management. You will learn why organizations face difficulties managing multiple AI providers (Amazon Bedrock, SageMaker, OpenAI, Anthropic...) with challenges including provider fragmentation, decentralized governance, operational complexity, cost management, and security. The solution uses LiteLLM (open source) combined with AWS services to create a centralized gateway. The article guides you through deployment options (ECS, EKS), network architectures (public global, regional, private VPC), and comprehensive governance features including: user/team management, API keys, budget control, load balancing, failover, rate limiting, model access control, and CloudWatch integration for monitoring. The gateway also supports Amazon SageMaker to expand custom model access capabilities, providing a unified, secure, and highly scalable AI governance solution.

###  [Blog 5 - How Smartsheet boosts developer productivity with Amazon Bedrock and Roo Codes](3.5-Blog5/)
This blog introduces how Smartsheet boosts developer productivity with Amazon Bedrock and Roo Code. You will learn about Roo Code - an autonomous AI coding assistant integrated directly into the editor, and Amazon Bedrock prompt caching - a technology that caches frequently used prompts, reducing costs by up to 90% and latency by 85%. Smartsheet achieved a 60% reduction in operational costs and 20% improvement in latency. The article presents real-world case studies: generating code documentation in 4 hours instead of 2 weeks, building an AWS cost analysis tool in 30 minutes that saves $450K annually. The solution works by separating static content (system prompts, code context) and dynamic content (user queries), caching static content for 5 minutes and reusing it for subsequent queries. Results show 70% of total input served from cache, with 83% cost reduction for follow-up queries. The article also shares best practices including: early caching implementation, developer experience optimization, and continuous monitoring.

###  [Blog 6 - Use generative AI on AWS to efficiently analyze clinical documents](3.6-Blog6/)
English translation:
This blog introduces how to use generative AI on AWS to efficiently analyze clinical documents. You will learn about challenges in the pharmaceutical industry: drug approval takes 10-12 years with 1 year for study startup, reviewing complex protocol documents often takes weeks or months. Clario - a provider of endpoint data solutions for clinical trials - built an AI platform on AWS based on four pillars: Parsing (using Amazon Textract, Comprehend to extract text/images/tables), Retrieval (using embedding models and Amazon OpenSearch Service vector database), Prompting (applying zero-shot/few-shot learning with Amazon Bedrock), and Generation (selecting appropriate LLMs to generate structured output). The solution architecture combines Amazon EC2, EKS, S3, SageMaker, Lambda, RDS, and Bedrock to process thousands of documents in seconds. The article also shares best practices: incremental and iterative development, evaluate models with test sets, optimize the 4 pillars before fine-tuning, and customize approach for specific use cases.